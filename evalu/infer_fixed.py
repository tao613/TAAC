#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆæ¨ç†è„šæœ¬
ä¸“é—¨å¤„ç†è‡ªé€‚åº”ç›¸ä¼¼åº¦æ¨¡å—æƒé‡åŠ è½½é—®é¢˜
"""

import argparse
import json
import os
import struct
from pathlib import Path

import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm

from dataset import MyTestDataset, save_emb, load_mm_emb
from model import BaselineModel


def get_ckpt_path():
    """è·å–è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„"""
    ckpt_path = os.environ.get("MODEL_OUTPUT_PATH")
    if ckpt_path is None:
        raise ValueError("MODEL_OUTPUT_PATH is not set")
    
    for item in os.listdir(ckpt_path):
        if item.endswith(".pt"):
            return os.path.join(ckpt_path, item)


def get_args():
    """è§£æè¯„ä¼°æ¨ç†çš„å‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser()

    # åŸºç¡€å‚æ•°
    parser.add_argument('--batch_size', default=128, type=int)
    parser.add_argument('--lr', default=0.001, type=float)
    parser.add_argument('--maxlen', default=101, type=int)
    parser.add_argument('--hidden_units', default=32, type=int)
    parser.add_argument('--num_blocks', default=1, type=int)
    parser.add_argument('--num_epochs', default=3, type=int)
    parser.add_argument('--num_heads', default=1, type=int)
    parser.add_argument('--dropout_rate', default=0.2, type=float)
    parser.add_argument('--l2_emb', default=0.0, type=float)
    parser.add_argument('--device', default='cuda', type=str)
    parser.add_argument('--inference_only', action='store_true')
    parser.add_argument('--state_dict_path', default=None, type=str)
    parser.add_argument('--norm_first', action='store_true', default=True)
    parser.add_argument('--mm_emb_id', nargs='+', default=['81'], type=str)
    
    # ä¼˜åŒ–åŠŸèƒ½å‚æ•°ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
    parser.add_argument('--use_adaptive_similarity', action='store_true')
    parser.add_argument('--similarity_types', nargs='+', default=['dot', 'cosine'])
    parser.add_argument('--use_smart_sampling', action='store_true')
    parser.add_argument('--use_curriculum_learning', action='store_true')

    return parser.parse_args()


def smart_load_model(model, checkpoint_path, device):
    """
    æ™ºèƒ½åŠ è½½æ¨¡å‹æƒé‡ï¼Œè‡ªåŠ¨å¤„ç†ç»“æ„ä¸åŒ¹é…é—®é¢˜
    """
    print(f"Loading checkpoint from: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªé€‚åº”ç›¸ä¼¼åº¦æ¨¡å—
    adaptive_keys = [k for k in checkpoint.keys() if k.startswith('adaptive_similarity.')]
    
    if adaptive_keys:
        print(f"æ£€æµ‹åˆ°è‡ªé€‚åº”ç›¸ä¼¼åº¦æƒé‡: {adaptive_keys}")
        
        # æ ¹æ®æƒé‡æ¨æ–­ç›¸ä¼¼åº¦ç±»å‹
        similarity_types = ['dot']
        if any('beta' in k for k in adaptive_keys):
            similarity_types.append('cosine')
        if any('gamma' in k for k in adaptive_keys):
            similarity_types.append('scaled')
        if any('bilinear' in k for k in adaptive_keys):
            similarity_types.append('bilinear')
            
        print(f"æ¨æ–­çš„ç›¸ä¼¼åº¦ç±»å‹: {similarity_types}")
        
        # æ·»åŠ è‡ªé€‚åº”ç›¸ä¼¼åº¦æ¨¡å—
        from model import AdaptiveSimilarity
        model.adaptive_similarity = AdaptiveSimilarity(
            model.hidden_units, 
            similarity_types=similarity_types
        ).to(device)
        print("å·²æ·»åŠ è‡ªé€‚åº”ç›¸ä¼¼åº¦æ¨¡å—")
    
    # ä½¿ç”¨strict=FalseåŠ è½½æƒé‡ï¼Œå¿½ç•¥ä¸åŒ¹é…çš„é”®
    missing_keys, unexpected_keys = model.load_state_dict(checkpoint, strict=False)
    
    if missing_keys:
        print(f"æ¨¡å‹ä¸­ç¼ºå¤±çš„æƒé‡ï¼ˆå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼‰: {missing_keys}")
    if unexpected_keys:
        print(f"checkpointä¸­å¤šä½™çš„æƒé‡ï¼ˆå·²å¿½ç•¥ï¼‰: {unexpected_keys}")
    
    print("âœ… æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")
    return model


def get_candidate_emb(indexer, feat_types, feat_default_value, mm_emb_dict, model):
    """ç”Ÿæˆå€™é€‰ç‰©å“embedding"""
    print("æ­£åœ¨ç”Ÿæˆå€™é€‰ç‰©å“embedding...")
    
    all_item_embs = []
    retrieve_id2creative_id = {}
    
    count = 0
    for item_reid, item_id in tqdm(indexer.items(), desc="Processing items"):
        if item_reid == 0:
            continue
            
        # å‡†å¤‡ç‰©å“ç‰¹å¾
        feat = {}
        
        # å¡«å……é»˜è®¤ç‰¹å¾
        for feat_type in feat_types['item_sparse']:
            feat[feat_type] = feat_default_value[feat_type]
        for feat_type in feat_types['item_array']:
            feat[feat_type] = feat_default_value[feat_type]
        for feat_type in feat_types['item_continual']:
            feat[feat_type] = feat_default_value[feat_type]
        
        # å¤šæ¨¡æ€ç‰¹å¾
        for feat_type in feat_types['item_emb']:
            if item_id in mm_emb_dict[feat_type]:
                feat[feat_type] = mm_emb_dict[feat_type][item_id]
            else:
                feat[feat_type] = feat_default_value[feat_type]
        
        # ç”Ÿæˆembedding
        with torch.no_grad():
            item_emb = model.predict_item(feat)
            all_item_embs.append(item_emb.detach().cpu().numpy().astype(np.float32))
        
        retrieve_id2creative_id[count] = item_id
        count += 1
    
    # ä¿å­˜å€™é€‰åº“embedding
    all_item_embs = np.concatenate(all_item_embs, axis=0)
    candidate_path = Path(os.environ.get('EVAL_RESULT_PATH'), 'candidate.fbin')
    save_emb(all_item_embs, candidate_path)
    
    # ä¿å­˜IDæ˜ å°„
    id_map_path = Path(os.environ.get('EVAL_RESULT_PATH'), 'retrieve_id2creative_id.json')
    with open(id_map_path, 'w') as f:
        json.dump(retrieve_id2creative_id, f)
    
    print(f"âœ… å€™é€‰åº“embeddingå·²ä¿å­˜: {all_item_embs.shape}")
    return retrieve_id2creative_id


def infer():
    """ä¸»æ¨ç†å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ¨ç†...")
    
    # å‚æ•°é…ç½®
    args = get_args()
    data_path = os.environ.get('EVAL_DATA_PATH')
    
    # æ•°æ®åŠ è½½
    print("ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")
    test_dataset = MyTestDataset(data_path, args)
    test_loader = DataLoader(
        test_dataset, batch_size=args.batch_size, shuffle=False, 
        num_workers=0, collate_fn=test_dataset.collate_fn
    )
    
    usernum, itemnum = test_dataset.usernum, test_dataset.itemnum
    feat_statistics = test_dataset.feat_statistics
    feat_types = test_dataset.feature_types
    
    print(f"ç”¨æˆ·æ•°: {usernum}, ç‰©å“æ•°: {itemnum}")
    
    # æ¨¡å‹åŠ è½½
    print("ğŸ¤– åˆ›å»ºå¹¶åŠ è½½æ¨¡å‹...")
    model = BaselineModel(usernum, itemnum, feat_statistics, feat_types, args).to(args.device)
    
    # æ™ºèƒ½åŠ è½½æƒé‡
    ckpt_path = get_ckpt_path()
    model = smart_load_model(model, ckpt_path, args.device)
    model.eval()
    
    # ç”Ÿæˆç”¨æˆ·query embedding
    print("ğŸ‘¤ ç”Ÿæˆç”¨æˆ·query embedding...")
    all_embs = []
    user_list = []
    
    for step, batch in tqdm(enumerate(test_loader), total=len(test_loader)):
        seq, token_type, seq_feat, user_id = batch
        seq = seq.to(args.device)
        
        with torch.no_grad():
            logits = model.predict(seq, seq_feat, token_type)
            
        for i in range(logits.shape[0]):
            emb = logits[i].unsqueeze(0).detach().cpu().numpy().astype(np.float32)
            all_embs.append(emb)
        user_list += user_id
    
    # ç”Ÿæˆå€™é€‰åº“embedding
    print("ğŸ¯ ç”Ÿæˆå€™é€‰åº“embedding...")
    retrieve_id2creative_id = get_candidate_emb(
        test_dataset.indexer['i'],
        test_dataset.feature_types,
        test_dataset.feature_default_value,
        test_dataset.mm_emb_dict,
        model,
    )
    
    # ä¿å­˜query embedding
    all_embs = np.concatenate(all_embs, axis=0)
    query_path = Path(os.environ.get('EVAL_RESULT_PATH'), 'query.fbin')
    save_emb(all_embs, query_path)
    
    print(f"âœ… Query embeddingå·²ä¿å­˜: {all_embs.shape}")
    print(f"âœ… æ¨ç†å®Œæˆï¼Œå…±å¤„ç† {len(user_list)} ä¸ªç”¨æˆ·")
    
    return None, user_list


if __name__ == "__main__":
    try:
        infer()
        print("ğŸ‰ æ¨ç†æˆåŠŸå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
